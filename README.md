# Nested Named Entity Recognition via Second-best Sequence Learning and Decoding (RU rework)

Основано на
**Nested Named Entity Recognition via Second-best Sequence Learning and Decoding** <br>
Takashi Shibuya, Eduard Hovy<br> 
[Статья](https://doi.org/10.1162/tacl_a_00334) и соответствующий [репозиторий](https://github.com/yahshibu/nested-ner-tacl2020-transformers).

<br>
<br>
а также
**Adaptation of Deep Bidirectional Multilingual Transformers for Russian Language** <br>
Yuri Kuratov, Mikhail Arkhipov ([Статья](https://arxiv.org/abs/1905.07213))
и самой [модели](https://huggingface.co/DeepPavlov/rubert-base-cased).<br>

Запуск и полное использование полностью повторяется за оригинальным репозиторием. 
Также в каталоге ```dumps``` можно найти пример обученной модели на данных ```NEREL-bio``` - веса обученной модели. Там же хранятся результаты обучения и тестирования модели. 

---

Based upon
**Nested Named Entity Recognition via Second-best Sequence Learning and Decoding** <br>
Takashi Shibuya, Eduard Hovy<br>
[Paper](https://doi.org/10.1162/tacl_a_00334) and related [repo](https://github.com/yahshibu/nested-ner-tacl2020-transformers).

<br>
<br>
as well as
**Adaptation of Deep Bidirectional Multilingual Transformers for Russian Language** <br>
Yuri Kuratov, Mikhail Arkhipov ([Paper](https://arxiv.org/abs/1905.07213))
and the [model](https://huggingface.co/DeepPavlov/rubert-base-cased).<br>

Launch and full use is completely the same as the original repository.
Also in the directory ```dumps``` you can find an example of a trained model on the data ```NEREL-bio``` - the weights of the trained model. The results of training and testing the model are also stored there.